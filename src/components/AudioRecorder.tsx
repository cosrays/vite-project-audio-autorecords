import { useState, useRef, useEffect } from 'react';

const AudioRecorder = () => {
  const [isRecording, setIsRecording] = useState(false);
  const [isMicrophoneBlocked, setIsMicrophoneBlocked] = useState(false);
  const [recordings, setRecordings] = useState<any[]>([]);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const audioStreamRef = useRef<MediaStream | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const silenceTimerRef = useRef<ReturnType<typeof setTimeout> | null>(null);
  const lastSoundTimeRef = useRef(Date.now());
  const recordingStartTimeRef = useRef<number | null>(null);
  const audioHistoryRef = useRef<{ timestamp: number; maxDb: number }[]>([]);

  // é…ç½®å‚æ•°
  const SILENCE_THRESHOLD = 0.1; // é™éŸ³é˜ˆå€¼
  const CHECK_INTERVAL = 200; // æ£€æŸ¥é—´éš” (ms)
  const SILENCE_DURATION = 2000; // é™éŸ³æŒç»­æ—¶é—´ (ms)

  const isValidVoiceRef = useRef(false);

  // è¯·æ±‚éº¦å…‹é£æƒé™
  const getMicrophonePermission = async () => {
    if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
      alert('æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒå½•éŸ³åŠŸèƒ½ (getUserMedia API not found).');
      setIsMicrophoneBlocked(true);
      return null;
    }
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      setIsMicrophoneBlocked(false);
      return stream;
    } catch (err: any) {
      console.error("è·å–éº¦å…‹é£æƒé™å¤±è´¥:", err);
      if (err.name === 'NotAllowedError' || err.name === 'PermissionDeniedError') {
        alert('æ‚¨å·²é˜»æ­¢éº¦å…‹é£æƒé™ã€‚è¯·åœ¨æµè§ˆå™¨è®¾ç½®ä¸­å…è®¸è®¿é—®éº¦å…‹é£ã€‚');
      } else if (err.name === 'NotFoundError' || err.name === 'DevicesNotFoundError') {
        alert('æœªæ‰¾åˆ°éº¦å…‹é£è®¾å¤‡ã€‚');
      } else {
        alert(`è·å–éº¦å…‹é£æƒé™æ—¶å‘ç”Ÿé”™è¯¯: ${err.message}`);
      }
      setIsMicrophoneBlocked(true);
      return null;
    }
  };

  // åˆå§‹åŒ–éŸ³é¢‘åˆ†æ
  const initAudioAnalysis = (stream: MediaStream) => {
    if (audioContextRef.current) {
      audioContextRef.current.close();
    }

    audioContextRef.current = new AudioContext();
    const source = audioContextRef.current.createMediaStreamSource(stream);
    analyserRef.current = audioContextRef.current.createAnalyser();
    analyserRef.current.fftSize = 2048;
    source.connect(analyserRef.current);
  };

  // æ£€æŸ¥éŸ³é¢‘çº§åˆ«
  const checkAudioLevel = () => {
    if (!analyserRef.current) return;

    const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount);
    analyserRef.current.getByteTimeDomainData(dataArray);

    // è·å–å½“å‰æ—¶åˆ»çš„æœ€å¤§éŸ³é‡

    const currentMaxDb = dataArray.reduce(function(h, k) {
      return Math.max(h, Math.abs(k - 127))
    }, 0) / 128;
    const now = Date.now();

    // å°†å½“å‰æ•°æ®æ·»åŠ åˆ°å†å²è®°å½•
    audioHistoryRef.current.push({ timestamp: now, maxDb: currentMaxDb });

    // ç§»é™¤è¶…è¿‡1ç§’çš„å†å²æ•°æ®
    audioHistoryRef.current = audioHistoryRef.current.filter(
      entry => now - entry.timestamp <= SILENCE_DURATION
    );

    // è®¡ç®—æœ€è¿‘1ç§’å†…çš„æœ€å¤§åˆ†è´
    const maxDbInLastSecond = audioHistoryRef.current.length > 0
      ? Math.max(...audioHistoryRef.current.map(entry => entry.maxDb))
      : currentMaxDb;

    console.log('å½“å‰æœ€å¤§éŸ³é‡ db:', currentMaxDb, 'æœ€è¿‘1ç§’å†…æœ€å¤§ db:', maxDbInLastSecond);

    if (maxDbInLastSecond > SILENCE_THRESHOLD) {
      isValidVoiceRef.current = true;
      lastSoundTimeRef.current = Date.now();
      // æœ€è¿‘1ç§’å†…éŸ³é‡è¶…è¿‡é˜ˆå€¼ï¼Œç»§ç»­å½•éŸ³å¹¶å»¶è¿Ÿæ£€æŸ¥
      lastSoundTimeRef.current = Date.now();

      silenceTimerRef.current = setTimeout(() => {
        checkAudioLevel();
      }, CHECK_INTERVAL);
    } else {
      if (Date.now() - lastSoundTimeRef.current >= SILENCE_DURATION) {
        // æœ€è¿‘1ç§’å†…éŸ³é‡éƒ½ä½äºé˜ˆå€¼ï¼Œç»“æŸå½“å‰å½•éŸ³å¹¶å¼€å§‹æ–°çš„å½•éŸ³
        console.log('æ£€æµ‹åˆ°é™éŸ³ï¼Œè‡ªåŠ¨åˆ†æ®µå½•éŸ³');
        // æ¸…ç©ºå†å²æ•°æ®
        audioHistoryRef.current = [];
        // handleCurrentRecording()
        stopRecording();
        startRecording(isValidVoiceRef.current);
      } else {
        silenceTimerRef.current = setTimeout(() => {
          checkAudioLevel();
        }, CHECK_INTERVAL);
      }
    }
  };

  // å°†Blobè½¬æ¢ä¸ºbase64çš„è¾…åŠ©å‡½æ•°
  const blobToBase64 = (blob: Blob) => {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onload = () => resolve(reader.result);
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  };

  // å¼€å§‹å½•éŸ³
  const startRecording = async (isValidVoice: boolean = false) => {
    const stream = await getMicrophonePermission();
    if (!stream) return;

    isValidVoiceRef.current = false;

    audioStreamRef.current = stream;
    audioChunksRef.current = [];
    lastSoundTimeRef.current = Date.now();
    recordingStartTimeRef.current = Date.now();
    // æ¸…ç©ºéŸ³é¢‘å†å²æ•°æ®
    audioHistoryRef.current = [];

    try {
      mediaRecorderRef.current = new MediaRecorder(stream);
      initAudioAnalysis(stream);

      mediaRecorderRef.current.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };

      mediaRecorderRef.current.onstop = async () => {
        console.log('isValidVoice', isValidVoice);
        // if (!isValidVoice) {
        //   return;
        // }
        const audioBlob = new Blob(audioChunksRef.current, { type: mediaRecorderRef.current?.mimeType || 'audio/wav' });
        const audioBase64 = await blobToBase64(audioBlob);
        const duration = (Date.now() - (recordingStartTimeRef.current || 0)) / 1000;
        setRecordings((prevRecordings: any) => [
          ...prevRecordings,
          {
            url: audioBase64,
            id: Date.now(),
            blob: audioBlob,
            name: `å½•éŸ³ ${prevRecordings.length + 1}`,
            duration: duration.toFixed(1)
          }
        ]);
      };

      mediaRecorderRef.current.onerror = (event) => {
        console.error("MediaRecorder é”™è¯¯:", event.error);
        alert(`å½•éŸ³è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: ${event.error.name}`);
        setIsRecording(false);
        audioStreamRef.current?.getTracks().forEach(track => track.stop());
        audioStreamRef.current = null;
      };

      mediaRecorderRef.current.start();
      setIsRecording(true);
      console.log("å½•éŸ³å¼€å§‹");

      lastSoundTimeRef.current = Date.now();
      silenceTimerRef.current = setTimeout(() => {
        checkAudioLevel()
      }, CHECK_INTERVAL);
    } catch (err: any) {
      console.error("åˆ›å»º MediaRecorder å¤±è´¥:", err);
      alert(`å¯åŠ¨å½•éŸ³å¤±è´¥: ${err.message}`);
      audioStreamRef.current?.getTracks().forEach(track => track.stop());
      audioStreamRef.current = null;
    }
  };

  const handleCurrentRecording = async () => {
    const audioBlob = new Blob(audioChunksRef.current, { type: mediaRecorderRef.current?.mimeType || 'audio/wav' });
    const audioBase64 = await blobToBase64(audioBlob);
    const duration = (Date.now() - (recordingStartTimeRef.current || 0)) / 1000;

    audioChunksRef.current = [];
    lastSoundTimeRef.current = Date.now();
    recordingStartTimeRef.current = Date.now();
    // æ¸…ç©ºéŸ³é¢‘å†å²æ•°æ®
    audioHistoryRef.current = [];
    setRecordings((prevRecordings: any) => [
      ...prevRecordings,
      {
        url: audioBase64,
        id: Date.now(),
        blob: audioBlob,
        name: `å½•éŸ³ ${prevRecordings.length + 1}`,
        duration: duration.toFixed(1)
      }
    ]);
  }

  // åœæ­¢å½•éŸ³
  const stopRecording = () => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === "recording") {
      mediaRecorderRef.current.stop();
      setIsRecording(false);
      console.log("å½•éŸ³åœæ­¢");

      if (audioStreamRef.current) {
        audioStreamRef.current.getTracks().forEach(track => track.stop());
        audioStreamRef.current = null;
        console.log("éº¦å…‹é£è½¨é“å·²åœæ­¢");
      }

      if (audioContextRef.current) {
        audioContextRef.current.close();
        audioContextRef.current = null;
      }

      if (silenceTimerRef.current) {
        clearTimeout(silenceTimerRef.current);
        silenceTimerRef.current = null;
      }
    }
  };

  // æ¸…ç†: ç»„ä»¶å¸è½½æ—¶é‡Šæ”¾èµ„æº
  useEffect(() => {
    return () => {
      if (audioStreamRef.current) {
        audioStreamRef.current.getTracks().forEach(track => track.stop());
      }
      if (mediaRecorderRef.current && mediaRecorderRef.current.state === "recording") {
        mediaRecorderRef.current.stop();
      }
      if (audioContextRef.current) {
        audioContextRef.current.close();
      }
      if (silenceTimerRef.current) {
        clearTimeout(silenceTimerRef.current);
      }
    };
  }, []);

  const handleDeleteRecording = (idToDelete: number) => {
    setRecordings(prevRecordings =>
      prevRecordings.filter((record: any) => {
        if (record.id === idToDelete) {
          return false;
        }
        return true;
      })
    );
  };

  return (
    <div className="h-full flex flex-column" style={{ fontFamily: 'Arial, sans-serif', padding: '20px', maxWidth: '600px', margin: 'auto' }}>
      <h2 style={{ textAlign: 'center', color: '#333' }}>React è‡ªåŠ¨å½•éŸ³æœº</h2>

      <div style={{ display: 'flex', justifyContent: 'center', marginBottom: '20px', gap: '10px' }}>
        {!isRecording ? (
          <button
            onClick={() => startRecording()}
            disabled={isMicrophoneBlocked}
            style={{
              padding: '10px 20px',
              fontSize: '16px',
              backgroundColor: '#4CAF50',
              color: 'white',
              border: 'none',
              borderRadius: '5px',
              cursor: 'pointer',
              opacity: isMicrophoneBlocked ? 0.5 : 1,
            }}
          >
            å¼€å§‹å½•éŸ³
          </button>
        ) : (
          <button
            onClick={stopRecording}
            style={{
              padding: '10px 20px',
              fontSize: '16px',
              backgroundColor: '#f44336',
              color: 'white',
              border: 'none',
              borderRadius: '5px',
              cursor: 'pointer',
            }}
          >
            åœæ­¢å½•éŸ³
          </button>
        )}
      </div>

      {isRecording && (
        <p style={{ textAlign: 'center', color: 'red', fontWeight: 'bold' }}>
          ğŸ”´ æ­£åœ¨å½•éŸ³ä¸­... (æ£€æµ‹åˆ°é™éŸ³å°†è‡ªåŠ¨åˆ†æ®µ)
        </p>
      )}
      {isMicrophoneBlocked && (
        <p style={{ textAlign: 'center', color: 'orange', fontWeight: 'bold' }}>
          éº¦å…‹é£æƒé™è¢«é˜»æ­¢æˆ–è®¾å¤‡ä¸å¯ç”¨ã€‚è¯·æ£€æŸ¥æµè§ˆå™¨è®¾ç½®ã€‚
        </p>
      )}

      <h3 style={{ marginTop: '30px', borderBottom: '1px solid #eee', paddingBottom: '10px', color: '#555' }}>
        æˆ‘çš„å½•éŸ³ ({recordings.length})
      </h3>
      {recordings.length === 0 && !isRecording && (
        <p style={{ textAlign: 'center', color: '#777' }}>æš‚æ— å½•éŸ³ã€‚ç‚¹å‡»"å¼€å§‹å½•éŸ³"æ¥åˆ›å»ºæ‚¨çš„ç¬¬ä¸€ä¸ªå½•éŸ³ï¼</p>
      )}
      <div className="flex-1 overflow-y-auto">
        <ul style={{ listStyle: 'none', padding: 0 }}>
          {recordings.slice().reverse().map((record: any) => (
            <li
              key={record.id}
              style={{
                display: 'flex',
                alignItems: 'center',
                justifyContent: 'space-between',
                padding: '10px',
                border: '1px solid #ddd',
                borderRadius: '4px',
                marginBottom: '10px',
                backgroundColor: parseFloat(record.duration) > 2.3 ? '#e6f3ff' : '#f9f9f9'
              }}
            >
              <div style={{ display: 'flex', alignItems: 'center', gap: '10px' }}>
                <span style={{ color: '#333' }}>{record.name}</span>
                <span style={{ color: '#666', fontSize: '0.9em' }}>
                  {new Date(record.id).toLocaleTimeString()} - {record.duration}ç§’
                </span>
              </div>
              <div style={{ display: 'flex', alignItems: 'center', gap: '10px' }}>
                <audio src={record.url} controls style={{ flexGrow: 1 }} />
                <button
                  onClick={() => handleDeleteRecording(record.id)}
                  style={{
                    backgroundColor: '#e74c3c',
                    color: 'white',
                    border: 'none',
                    borderRadius: '4px',
                    padding: '5px 10px',
                    cursor: 'pointer'
                  }}
                >
                  åˆ é™¤
                </button>
              </div>
            </li>
          ))}
        </ul>
      </div>
    </div>
  );
};

export default AudioRecorder;